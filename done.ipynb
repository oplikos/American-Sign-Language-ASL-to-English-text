{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56253627",
   "metadata": {},
   "source": [
    "\n",
    "# <u> ASL Translator </u> #\n",
    "\n",
    "The ASL Translator project is aimed at developing a machine learning-based system that translates American Sign Language (ASL) gestures into English. This project utilizes a dataset found on Kaggle, which provides a collection of ASL alphabet images for training and testing.\n",
    "\n",
    "---\n",
    "\n",
    "## Team Member ##\n",
    "- Sarkis Bouzikian\n",
    "- Ryan\n",
    "- Conar\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objective ##\n",
    "The primary objective of this project is to create a runnable program that can accurately interpret ASL signs captured through images, videos, and live feeds in real-time. The program is designed to assist deaf and hard-of-hearing individuals in understanding and communicating in classroom interactions. The traditional approach of relying on human translators to interpret sign language word by word can be challenging, and this project aims to provide an automated solution.\n",
    "\n",
    "---\n",
    "\n",
    "## ASL Detection and Translation ## \n",
    "ASL detection involves recognizing and understanding the hand gestures and movements that represent individual letters or words in American Sign Language. The project leverages machine learning techniques to train a model that can accurately classify ASL gestures. The model is built using the Keras library and follows a convolutional neural network (CNN) architecture.\n",
    "\n",
    "The training data consists of images of ASL alphabet gestures, with each image labeled with the corresponding letter. The dataset is split into training and testing sets to evaluate the model's performance. During training, the model learns to recognize patterns and features in the images, allowing it to classify new, unseen gestures.\n",
    "\n",
    "Once the model is trained, it can be used to detect and translate ASL gestures in real-time. The program captures video frames, applies hand detection using the Mediapipe library, and then passes the detected hand landmarks to the trained model for classification. The program displays the recognized gesture on the screen, allowing users to understand the translated ASL letter or word.\n",
    "\n",
    "---\n",
    "\n",
    "## Program Features and GUI ##\n",
    "The ASL Translator program includes a graphical user interface (GUI) that provides a user-friendly environment for interaction. The GUI displays a video feed from the webcam or other input source, along with the translated ASL gesture and an accompanying image of the corresponding ASL letter.\n",
    "\n",
    "The program allows users to switch between different ASL images, providing a comprehensive set of gestures for translation. It also includes a slider to control the gesture threshold, enabling users to adjust the sensitivity of the ASL detection.\n",
    "\n",
    "---\n",
    "\n",
    "## Using Kaggle Dataset for ASL Translator ##\n",
    "In the ASL Translator project, we have chosen to utilize a dataset found on Kaggle instead of creating our own dataset. Kaggle is a popular platform for hosting and sharing datasets, and it provides a wide range of datasets across various domains.\n",
    "\n",
    "By leveraging the ASL alphabet dataset available on Kaggle, we can expedite the development process and benefit from a pre-existing collection of ASL gesture images. The dataset on Kaggle consists of labeled images representing each letter of the American Sign Language alphabet.\n",
    "\n",
    "Using an established dataset like this allows us to take advantage of the efforts of the data contributors, who have meticulously labeled and organized the ASL gesture images. This saves us significant time and resources that would have been required to create a similar dataset from scratch.\n",
    "\n",
    "The Kaggle dataset offers a diverse set of ASL alphabet gestures, including different hand shapes, orientations, and backgrounds. This variety helps improve the robustness and generalization capability of the machine learning model we develop for ASL detection and translation.\n",
    "\n",
    "By using the Kaggle dataset, we can focus more on the development of the machine learning model architecture, training process, and real-time translation implementation. We can utilize the labeled dataset to train the model, evaluate its performance, and fine-tune its parameters to achieve higher accuracy in recognizing ASL gestures.\n",
    "\n",
    "Overall, incorporating the Kaggle dataset into our ASL Translator project streamlines the development process and enhances the project's reliability and effectiveness. It demonstrates the power of open datasets and collaborative platforms like Kaggle in fostering innovation and accelerating the progress of machine learning applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion ##\n",
    "The ASL Translator project demonstrates the potential of machine learning and computer vision techniques in improving communication accessibility for deaf and hard-of-hearing individuals. By automating the translation of ASL gestures into English, this project aims to assist in classroom interactions and facilitate better understanding and communication.\n",
    "\n",
    "Through the development of a functional ASL-to-English interpreter with high accuracy, this project showcases the feasibility of using machine learning models trained on ASL datasets to bridge the communication gap and enhance accessibility for the deaf and hard-of-hearing community.\n",
    "\n",
    "Please note that to run this project, you need to have the required libraries installed and ensure the ASL alphabet dataset is available in the specified training directory.\n",
    "\n",
    "---\n",
    "\n",
    "## Source ##\n",
    "\n",
    "https://chat.openai.com/<br> \n",
    "https://teachablemachine.withgoogle.com/train<br>\n",
    "https://github.com/computervisioneng/sign-language-detector-python<br>\n",
    "https://github.com/datamagic2020/sign-language-detection<br>\n",
    "https://github.com/albanjoseph/Signapse<br>\n",
    "https://github.com/shylagangwar1911/ASL-to-English-Translation/tree/master<br>\n",
    "https://github.com/gerardodekay/Real-time-ASL-to-English-text-translation<br>\n",
    "https://github.com/Sahiljawale/Real-Time-Sign-language-recognition-system<br>\n",
    "https://clear.ml/<br>\n",
    "https://www.youtube.com/watch?v=wa2ARoUUdU8&ab_channel=Murtaza%27sWorkshop-RoboticsandAI<br>\n",
    "https://github.com/murtazahassan<br>\n",
    "https://www.computervision.zone/courses/hand-sign-detection-asl/<br>\n",
    "https://github.com/HaibaoBaba/Sign-Language-Translator-ASL<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Required installations: ##\n",
    "\n",
    "numpy<br>\n",
    "mediapipe<br>\n",
    "opencv-python<br>\n",
    "matplotlib<br>\n",
    "tensorflow<br>\n",
    "scikit-learn<br>\n",
    "pyspellchecker<br>\n",
    "\n",
    "With these installations, you can execute the provided Jupyter Notebook code to run the ASL Translator program.<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requerment pip install (the following tools) (recomanded to start in new env ()) ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34638555",
   "metadata": {},
   "source": [
    "numpy\n",
    "mediapipe\n",
    "tensorflow\n",
    "opencv-python\n",
    "pyspellchecker\n",
    "scikit-learn\n",
    "matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2541ac25",
   "metadata": {},
   "source": [
    "#### import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageTk, Image\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Auto_Correct_SpellChecker import Auto_Correct\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import ImageTk, Image\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef360e3c",
   "metadata": {},
   "source": [
    "#### skip this if you have trained modual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    cap = None\n",
    "    ## Load the ASL alphabet training images\n",
    "    training_dir = r'./asl_alphabet_train'\n",
    "    data_gatherer = DataGatherer(training_dir)\n",
    "    x_train, x_test, y_train, y_test = data_gatherer.load_images()\n",
    "    ## Define the batch size and compute steps per epoch\n",
    "    batch_size = 64\n",
    "    training_size = x_train.shape[0]\n",
    "    test_size = x_test.shape[0]\n",
    "    compute_steps_per_epoch = lambda x: int(np.ceil(1. * x / batch_size))\n",
    "    steps_per_epoch = compute_steps_per_epoch(training_size)\n",
    "    val_steps = compute_steps_per_epoch(test_size)\n",
    "    ## Build the model\n",
    "    classifier = Model(Sequential()).classifier\n",
    "    classifier = Model.build_model(classifier)\n",
    "    ## Compile the model\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ## Train the model\n",
    "    history = classifier.fit(\n",
    "        x_train, y_train,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=15,\n",
    "        validation_data=(x_test, y_test),\n",
    "        validation_steps=val_steps\n",
    "    )\n",
    "    ## Plot accuracy graph\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"classifier\")\n",
    "    plt.show()\n",
    "    ## Save the trained model\n",
    "    Model.save_classifier('grayscale_classifier1', classifier)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb62fc8d",
   "metadata": {},
   "source": [
    "### skip to here dont run tain if you have modual pre trained "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd882171",
   "metadata": {},
   "source": [
    "### Define GUI class for the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUI:\n",
    "    \n",
    "    def __init__(self, title, size):\n",
    "        self.root = Tk()\n",
    "        self.root.title(title)\n",
    "        self.root.geometry(size)\n",
    "        self.root.attributes('-fullscreen', True)\n",
    "        \n",
    "    def create_frame(self, width, height, anchor, relx, rely, background='#37251b'):\n",
    "        frame = Frame(self.root, bg=background, width=width, height=height)\n",
    "        frame.place(anchor=anchor, relx=relx, rely=rely)\n",
    "        return frame\n",
    "        \n",
    "    def create_labels(self, label_num, labels, anchor, relx, rely, x_spacing=0, y_spacing=0, create_entrybox_per_label=False):\n",
    "        entry_labels = {}\n",
    "        entry_boxes = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "\n",
    "        longest_label_spacing = len(max(labels, key=len))/100.0\n",
    "        \n",
    "        for i in range(label_num):\n",
    "            label = Label(self.root, text = labels[i]+\": \",\n",
    "                           font = (\"TimesNewRoman\", 15))\n",
    "            label.place(anchor=anchor, relx=relx, rely=rely)\n",
    "            \n",
    "            entry_labels[labels[i]] = label\n",
    "            if create_entrybox_per_label:\n",
    "                entry_box = Text(self.root, font=(\"TimesNewRoman\", 20), height=1, width=10)\n",
    "                entry_box.place(anchor=anchor, relx=relx+longest_label_spacing+0.02, rely=rely)\n",
    "                \n",
    "                entry_boxes[labels[i]+'_entrybox'] = entry_box\n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "        return entry_labels, entry_boxes\n",
    "    def create_labels2(self, label_num, labels, anchor, relx, rely, x_spacing=0, y_spacing=0, create_entrybox_per_label=False, images=None):\n",
    "        entry_labels = {}\n",
    "        entry_boxes = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "\n",
    "        longest_label_spacing = len(max(labels, key=len))/100.0\n",
    "        \n",
    "        for i in range(label_num):\n",
    "            label = Label(self.root, text=labels[i]+\": \", font=(\"TimesNewRoman\", 15))\n",
    "            label.place(anchor=anchor, relx=relx, rely=rely)\n",
    "            \n",
    "            entry_labels[labels[i]] = label\n",
    "            if create_entrybox_per_label:\n",
    "                entry_box = Text(self.root, font=(\"TimesNewRoman\", 20), height=1, width=10)\n",
    "                entry_box.place(anchor=anchor, relx=relx+longest_label_spacing+0.02, rely=rely)\n",
    "                \n",
    "                entry_boxes[labels[i]+'_entrybox'] = entry_box\n",
    "\n",
    "            if images and i < len(images):\n",
    "                image_label = Label(self.root, image=images[i])\n",
    "                image_label.place(anchor=anchor, relx=relx+longest_label_spacing+0.2, rely=rely)\n",
    "            \n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "        return entry_labels, entry_boxes\n",
    "    def create_buttons(self, button_num, text, anchor, relx, rely, command=None, x_spacing=0, y_spacing=0):\n",
    "        buttons = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "        \n",
    "        for i in range(button_num):\n",
    "            btn = Button(self.root, command=command, text=text[i])\n",
    "            btn.place(anchor=anchor, relx=relx, rely=rely)\n",
    "\n",
    "            buttons[text[i]+' button'] = btn\n",
    "            \n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "\n",
    "        return buttons\n",
    "    def load_images_from_dir(self, dir_path, image_size):\n",
    "        images = []\n",
    "        for file in os.listdir(dir_path):\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                image_path = os.path.join(dir_path, file)\n",
    "                image = Image.open(image_path)\n",
    "                image = image.resize(image_size, Image.ANTIALIAS)\n",
    "                image_tk = ImageTk.PhotoImage(image)\n",
    "                images.append(image_tk)\n",
    "        return images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd15016e",
   "metadata": {},
   "source": [
    "#### Define class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "  classifier = None\n",
    "  def __init__(self, Type):\n",
    "    self.classifier = Type\n",
    "    \n",
    "  def build_model(classifier):\n",
    "    classifier.add(Convolution2D(128, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(1024, activation='relu'))\n",
    "    classifier.add(Dense(29, activation='softmax'))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "  def save_classifier(path, classifier):\n",
    "    classifier.save(path)\n",
    "\n",
    "  def load_classifier(path):\n",
    "    classifier = load_model(path)\n",
    "    return classifier\n",
    "\n",
    "  def predict(classes, classifier, img):\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img/255.0\n",
    "\n",
    "    pred = classifier.predict(img)\n",
    "    return classes[np.argmax(pred)], pred\n",
    "    \n",
    "\n",
    "class DataGatherer:\n",
    "\n",
    "  def __init__(self, *args):\n",
    "    if len(args) > 0:\n",
    "      self.dir = args[0]\n",
    "    elif len(args) == 0:\n",
    "      self.dir = \"\"\n",
    "\n",
    "\n",
    "  ##this function loads the images along with their labels and apply\n",
    "  ##pre-processing function on the images and finaly split them into train and\n",
    "  ##test dataset\n",
    "  def load_images(self):\n",
    "    images = []\n",
    "    labels = []\n",
    "    index = -1\n",
    "    folders = sorted(os.listdir(self.dir))\n",
    "    \n",
    "    for folder in folders:\n",
    "      index += 1\n",
    "      \n",
    "      print(\"Loading images from folder \", folder ,\" has started.\")\n",
    "      for image in os.listdir(self.dir + '/' + folder):\n",
    "\n",
    "        img = cv2.imread(self.dir + '/' + folder + '/' + image, 0)\n",
    "        \n",
    "        img = self.edge_detection(img)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img_to_array(img)\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(index)\n",
    "\n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')/255.0\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "  def edge_detection(self, image):\n",
    "    minValue = 70\n",
    "    blur = cv2.GaussianBlur(image,(5,5),2)\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e20e57cf",
   "metadata": {},
   "source": [
    "#### Load the pre-trained model for ASL detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Model.load_classifier('grayscale_classifier1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df6c839e",
   "metadata": {},
   "source": [
    "#### Create the GUI object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui = GUI(\"ASL to English Translation\", \"800x600\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f321c56",
   "metadata": {},
   "source": [
    "#### test to load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd13fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "image_folder = 'test'\n",
    "images = load_images_from_folder(image_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcc2f20f",
   "metadata": {},
   "source": [
    "#### Create the video stream frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_frame = gui.create_frame(600, 600, 'ne', 1, 0, '#37251b')\n",
    "vid_label = Label(vid_frame)\n",
    "vid_label.grid(row=0, column=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da55a176",
   "metadata": {},
   "source": [
    "#### Load and display ASL images in a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (300, 300)\n",
    "images = gui.load_images_from_dir(\"test\", image_size)\n",
    "image_label = Label(vid_frame)\n",
    "image_label.grid(row=1, column=0, padx=20, pady=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64c271e",
   "metadata": {},
   "source": [
    "#### Initial image index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0\n",
    "update_image_label(image_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46c025cf",
   "metadata": {},
   "source": [
    "#### Function to update the image label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_label(image_index):\n",
    "    image_label.config(image=images[image_index])\n",
    "    image_label.image = images[image_index]\n",
    "\n",
    "## Function to switch between images\n",
    "def switch_image(step):\n",
    "    nonlocal image_index\n",
    "    image_index += step\n",
    "    if image_index < 0:\n",
    "        image_index = len(images) - 1\n",
    "    elif image_index >= len(images):\n",
    "        image_index = 0\n",
    "    update_image_label(image_index)\n",
    "\n",
    "## Button to switch to the next image\n",
    "next_image_button = Button(vid_frame, text=\"Next\", command=lambda: switch_image(1))\n",
    "next_image_button.grid(row=1, column=1, padx=10)\n",
    "\n",
    "## Button to switch to the previous image\n",
    "prev_image_button = Button(vid_frame, text=\"Previous\", command=lambda: switch_image(-1))\n",
    "prev_image_button.grid(row=1, column=2, padx=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2292e1b",
   "metadata": {},
   "source": [
    "#### Slider to control the gesture threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e12dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_label = Label(vid_frame, text=\"Gesture Threshold:\", font=(\"Arial\", 12), fg=\"white\", bg=\"#37251b\")\n",
    "threshold_label.grid(row=2, column=0, pady=10)\n",
    "threshold_slider = Scale(vid_frame, from_=0, to=100, orient=HORIZONTAL, length=400)\n",
    "threshold_slider.set(80)\n",
    "threshold_slider.grid(row=2, column=1, pady=10, padx=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba70ba7c",
   "metadata": {},
   "source": [
    "#### Function to perform ASL detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_asl():\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        results = hands.process(gray)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                prediction = Model.predict_gesture(classifier, hand_landmarks, gray, threshold_slider.get())\n",
    "                gesture_image = cv2.resize(images[image_index], (100, 100))\n",
    "                frame[10:110, 10:110] = gesture_image\n",
    "                cv2.putText(frame, prediction, (120, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "        \n",
    "        cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "        img = Image.fromarray(cv2image)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        vid_label.config(image=imgtk)\n",
    "        vid_label.imgtk = imgtk\n",
    "        gui.root.update()\n",
    "    \n",
    "    cap.release()\n",
    "    gui.root.destroy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09fd1477",
   "metadata": {},
   "source": [
    "#### draw_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e18758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_region(image, center):\n",
    "    cropped_image = cv2.rectangle(image, (center[0] - 130, center[1] - 130),\n",
    "        (center[0] + 130, center[1] + 130), (0, 0, 255), 2)\n",
    "    return cropped_image[center[1]-130:center[1]+130, center[0]-130:center[0]+130], cropped_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d81aacd0",
   "metadata": {},
   "source": [
    "#### start_gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_gui(title, size):\n",
    "    gui = GUI(title, size)\n",
    "\n",
    "    gui_frame = gui.create_frame(600, 600, 'ne', 1, 0, '#37251b')\n",
    "    vid_label = Label(gui_frame)\n",
    "    vid_label.grid(row=0, column=0)\n",
    "\n",
    "    ## Load and resize images\n",
    "    image_size = (300, 300)\n",
    "    images = gui.load_images_from_dir(\"test\", image_size)\n",
    "    image_names  = ['del', 'nothing', 'space','A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "           'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "           'W', 'X', 'Y', 'Z']\n",
    "    ## entry_labels, entry_boxes = gui.create_labels2(len(image_names), image_names, 's', 0.5, 0.3, images=images)\n",
    "    ## Create label to display images\n",
    "    image_label = Label(gui_frame)\n",
    "    image_label.grid(row=1, column=0, padx=20, pady=20)\n",
    "\n",
    "    ## Function to update the image label\n",
    "    def update_image_label(image_index):\n",
    "        image_label.config(image=images[image_index])\n",
    "        image_label.image = images[image_index]\n",
    "\n",
    "    ## Initial image index\n",
    "    image_index = 0\n",
    "    update_image_label(image_index)\n",
    "\n",
    "    ## Button to switch to the next image\n",
    "    next_image_button = Button(gui_frame, text=\"Next\", command=lambda: switch_image(1))\n",
    "    next_image_button.grid(row=1, column=1, padx=10)\n",
    "\n",
    "    ## Button to switch to the previous image\n",
    "    prev_image_button = Button(gui_frame, text=\"Previous\", command=lambda: switch_image(-1))\n",
    "    prev_image_button.grid(row=1, column=2, padx=10)\n",
    "\n",
    "    ## Function to switch between images\n",
    "    def switch_image(step):\n",
    "        nonlocal image_index\n",
    "        image_index += step\n",
    "        if image_index < 0:\n",
    "            image_index = len(images) - 1\n",
    "        elif image_index >= len(images):\n",
    "            image_index = 0\n",
    "        update_image_label(image_index)\n",
    "\n",
    "    return gui, vid_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8667c5a9",
   "metadata": {},
   "source": [
    "#### exit_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_app(gui, cap):\n",
    "    gui.root.destroy()\n",
    "    cap.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9fd06e",
   "metadata": {},
   "source": [
    "#### update_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frame(image, vid_label):\n",
    "    image_fromarray = Image.fromarray(image)\n",
    "    imgtk = ImageTk.PhotoImage(image=image_fromarray)\n",
    "    \n",
    "    vid_label.imgtk = imgtk\n",
    "    vid_label.config(image=imgtk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "594f53ff",
   "metadata": {},
   "source": [
    "#### get_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f434063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(label_entrybox):\n",
    "    value = label_entrybox.get('1.0', END)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return 0.95"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6813bdfb",
   "metadata": {},
   "source": [
    "#### get_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char(gesture):\n",
    "    classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "           'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "    return Model.predict(classes, classifier, gesture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c032483",
   "metadata": {},
   "source": [
    "#### Add Char To Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7580285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddCharToWord(word, curr_char):\n",
    "    temp_word = word\n",
    "    if curr_char == 'space':\n",
    "        ##print(Auto_Correct(temp_word))\n",
    "        temp_word = \"\"\n",
    "    elif curr_char == 'del':\n",
    "        temp_word = temp_word[0:-1]\n",
    "        print('character has been deleted')\n",
    "    elif curr_char != 'nothing':\n",
    "        temp_word += curr_char.lower()\n",
    "        print('character has been added: ', curr_char.lower())\n",
    "\n",
    "    return [temp_word, curr_char]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f63d3f12",
   "metadata": {},
   "source": [
    "#### frame_video_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32081f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_video_stream(names, curr_char, prev_char, word, sentence, *args):\n",
    "    kwargs = dict(zip(names, args))\n",
    "    \n",
    "    threshold = get_threshold(kwargs['th_box'])\n",
    "    curr_char = curr_char\n",
    "    prev_char = prev_char\n",
    "    \n",
    "    success, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    ## Flip the image horizontally for a later selfie-view display, and convert\n",
    "    ## the BGR image to RGB.\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    update_frame(image, kwargs['vid_label'])\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = kwargs['hands'].process(image)\n",
    "\n",
    "    ## Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "            y = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "\n",
    "            \n",
    "            center = np.array([np.mean(x) * image_width, np.mean(y) * image_height]).astype('int32')\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            cropped_img, full_img = draw_region(image, center)\n",
    "\n",
    "            update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "            try:\n",
    "                ##print('from try')\n",
    "                gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "                gray = DataGatherer().edge_detection(gray)\n",
    "\n",
    "                curr_char, pred = get_char(gray)\n",
    "                char = cv2.putText(full_img, curr_char, (center[0]-135, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                char_prob = cv2.putText(full_img, '{0:.2f}'.format(np.max(pred)), (center[0]+60, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "                kwargs['cc_box'].delete('1.0', 'end')\n",
    "                kwargs['cc_box'].insert('end', curr_char)\n",
    "        \n",
    "                if (curr_char != prev_char) and (np.max(pred) > threshold):\n",
    "                    ##the below print statement is related to the formatter\n",
    "                    ##print(pred)\n",
    "                    temp = AddCharToWord(word, curr_char)\n",
    "                    kwargs['ow_box'].insert('end', curr_char)\n",
    "                    \n",
    "                    if (temp[0] == \"\") and (temp[1] != \"del\"):\n",
    "                        sentence += Auto_Correct(word) + \" \"\n",
    "                        kwargs['sent_box'].insert('end', Auto_Correct(word) + \" \")\n",
    "                        kwargs['ow_box'].delete('1.0', 'end')\n",
    "                        kwargs['cw_box'].delete('1.0', 'end')\n",
    "                        kwargs['cw_box'].insert('end', Auto_Correct(word))\n",
    "                    word = temp[0]\n",
    "\n",
    "                    prev_char = curr_char\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    kwargs['vid_label'].after(1, frame_video_stream, names, curr_char, prev_char, word, sentence, *args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a17d2387",
   "metadata": {},
   "source": [
    "#### pipe_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_cam(gui, vid_label, images):\n",
    "\n",
    "    curr_char = None\n",
    "    prev_char = None\n",
    "    word = \"\"\n",
    "    sentence = \"\"\n",
    "    threshold = float(0.95)\n",
    "\n",
    "    float_formatter = \"{:.5f}\".format\n",
    "    np.set_printoptions(formatter={'float_kind': float_formatter})\n",
    "\n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    labels_num = 5\n",
    "    labels = ['threshold', 'current char',\n",
    "              'original word', 'corrected word', 'sentence']\n",
    "\n",
    "    Labels, entryboxes = gui.create_labels(\n",
    "        labels_num, labels, 'nw', 0, 0, y_spacing=0.06, create_entrybox_per_label=True)\n",
    "\n",
    "    entryboxes['threshold_entrybox'].config(width=8)\n",
    "    entryboxes['current char_entrybox'].config(width=8)\n",
    "    entryboxes['original word_entrybox'].config(width=25)\n",
    "    entryboxes['corrected word_entrybox'].config(width=25)\n",
    "    entryboxes['sentence_entrybox'].config(width=25, height=6)\n",
    "\n",
    "    Exit_program_btn = gui.create_buttons(\n",
    "        1, ['Exit'], 'center', 0.5, 0.9, command=lambda: exit_app(gui, cap))\n",
    "\n",
    "    names = ['vid_label', 'hands', 'th_box', 'cc_box',\n",
    "             'ow_box', 'cw_box', 's_box', 'exit_btn']\n",
    "    # gui.display_widgets(names)\n",
    "    th_entrybox = entryboxes['threshold_entrybox']\n",
    "\n",
    "    cc_entrybox = entryboxes['current char_entrybox']\n",
    "\n",
    "    ow_entrybox = entryboxes['original word_entrybox']\n",
    "\n",
    "    cw_entrybox = entryboxes['corrected word_entrybox']\n",
    "\n",
    "    sent_entrybox = entryboxes['sentence_entrybox']\n",
    "\n",
    "    Exit_program_btn = gui.create_buttons(\n",
    "        1, ['Exit'], 'center', 0.5, 0.9, command=lambda: exit_app(gui, cap))\n",
    "\n",
    "    names = ['vid_label', 'hands', 'th_box',\n",
    "             'cc_box', 'ow_box', 'cw_box', 'sent_box']\n",
    "    with mp_hands.Hands(\n",
    "            min_detection_confidence=0.4,\n",
    "            min_tracking_confidence=0.5,\n",
    "            max_num_hands=1) as hands:\n",
    "\n",
    "        frame_video_stream(names, curr_char, prev_char, word, sentence, vid_label,\n",
    "                           hands,  th_entrybox, cc_entrybox, ow_entrybox, cw_entrybox, sent_entrybox)\n",
    "        gui.root.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e37c5aa",
   "metadata": {},
   "source": [
    "#### Start of GUI ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Sign Language Recognition GUI\"\n",
    "size = \"1100x1100\"\n",
    "\n",
    "## gui, vid_label = start_gui(title, size,image)\n",
    "gui, vid_label = start_gui(title, size)\n",
    "pipe_cam(gui, vid_label, images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
